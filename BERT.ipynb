{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torchtext.datasets import AG_NEWS\n",
    "import random\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "# PATH = os.path.join(os.getcwd(), \"models\", \"bert-base-uncased\")\n",
    "# PATH = os.path.join(os.getcwd(), \"models\", \"bert_uncased_L-2_H-128_A-2\")\n",
    "PATH = os.path.join(os.getcwd(), \"models\", \"bert_uncased_L-8_H-512_A-8\")\n",
    "VOCAB_PATH = os.path.join(PATH, \"vocab.txt\")\n",
    "WEIGHTS_PATH = os.path.join(PATH, \"pytorch_model.bin\")\n",
    "# 加载bert的分词器\n",
    "TOKENIZER = BertTokenizer.from_pretrained(VOCAB_PATH)\n",
    "# 加载bert模型，这个路径文件夹下有bert_config.json配置文件和model.bin模型权重文件\n",
    "BERT = BertModel.from_pretrained(PATH)\n",
    "\n",
    "# s = \"I'm not sure, this can work, lol -.-\"\n",
    "\n",
    "# tokens = TOKENIZER.tokenize(s)\n",
    "# print(\"\\\\\".join(tokens))\n",
    "# # \"i\\\\'\\\\m\\\\not\\\\sure\\\\,\\\\this\\\\can\\\\work\\\\,\\\\lo\\\\##l\\\\-\\\\.\\\\-\"\n",
    "# # 是否需要这样做？\n",
    "# # tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "# ids = torch.tensor([TOKENIZER.convert_tokens_to_ids(tokens)])\n",
    "# print(ids.shape)\n",
    "# # torch.Size([1, 15])\n",
    "# print(ids)\n",
    "\n",
    "# result = BERT(ids)\n",
    "# print(len(result))\n",
    "# # torch.Size([1, 15, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "7600\n",
      "938\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "test_iter = AG_NEWS(split='test')\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "# convert 1-4 to 0-3\n",
    "for (label, line) in train_iter:\n",
    "    train_data.append([line, label-1])\n",
    "for (label, line) in test_iter:\n",
    "    test_data.append([line, label-1])\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, data, max_sequence_length=128):\n",
    "        self.data = data\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text, label = self.data[index]\n",
    "        tokens = TOKENIZER.tokenize(text)[:self.max_sequence_length-2]\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        input_ids = TOKENIZER.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # 将序列填充到max_sequence_length长度，并创建相应的attention_mask\n",
    "        padding_length = self.max_sequence_length - len(input_ids)\n",
    "        input_ids += [0] * padding_length\n",
    "        attention_mask += [0] * padding_length\n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(label)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = AGNewsDataset(train_data, max_sequence_length=128)\n",
    "test_dataset = AGNewsDataset(test_data, max_sequence_length=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTCLasifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 512)\n",
      "      (token_type_embeddings): Embedding(2, 512)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-7): 8 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n",
      "Model Summary:\n",
      "Layer: fc.weight | Size: torch.Size([4, 512]) | Parameters: 2048\n",
      "Layer: fc.bias | Size: torch.Size([4]) | Parameters: 4\n",
      "Total Trainable Parameters: 2052\n"
     ]
    }
   ],
   "source": [
    "class BERTCLasifier(nn.Module):\n",
    "    def __init__(self, bert, num_classes=4):\n",
    "        super(BERTCLasifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False  # 冻结BERT模型的权重\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        _, pooled_output = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return  self.fc(pooled_output)\n",
    "    \n",
    "\n",
    "model = BERTCLasifier(BERT, num_classes=4)\n",
    "\n",
    "def print_model_summary(model):\n",
    "    print(model)\n",
    "    print(\"Model Summary:\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name} | Size: {param.size()} | Parameters: {param.numel()}\")\n",
    "            total_params += param.numel()\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model found, training new model...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "model = BERTCLasifier(BERT, num_classes=4)\n",
    "MODEL_NAME = \"BERT_medium_\"\n",
    "RE_MODEL = re.compile(r\"BERT_medium_(\\d+).pth\")\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved at {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "def check_model(model, path):\n",
    "    if os.path.exists(path):\n",
    "        load_model(model, path)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_latest_model(path):\n",
    "    Latest_Model = RE_MODEL\n",
    "    files = os.listdir(path)\n",
    "    latest_model = 0\n",
    "    for file in files:\n",
    "        match = Latest_Model.match(file)\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            if epoch > latest_model:\n",
    "                latest_model = epoch\n",
    "    return latest_model\n",
    "\n",
    "latest_model_epoch = get_latest_model(os.getcwd())\n",
    "MODEL_PATH = os.path.join(os.getcwd(), MODEL_NAME + str(latest_model_epoch) + \".pth\")\n",
    "\n",
    "\n",
    "if check_model(model, MODEL_PATH):\n",
    "    print(\"Model found, loading mode:\", latest_model_epoch)\n",
    "    print_model_summary(model)\n",
    "else:\n",
    "    print(\"No model found, training new model...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch, print_every=10, writer=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch_idx, (input_ids, attention_mask, label) in enumerate(train_loader):\n",
    "        input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total_correct += (predicted == label).sum().item()\n",
    "\n",
    "        if (batch_idx+1) % (len(train_loader)//print_every) == 0:\n",
    "            print(f\"Train Epoch: {epoch} | {batch_idx * BATCH_SIZE} / {len(train_loader.dataset)} [{(100 * batch_idx/len(train_loader)):.0f} %] | Loss: {total_loss/(batch_idx+1):.3f} | Accuracy: {total_correct/(BATCH_SIZE*(batch_idx+1)):.3f}\")\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Loss/train\", total_loss/len(train_loader), epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", total_correct/len(train_loader), epoch)\n",
    "\n",
    "    print(f\"Train Epoch: {epoch} | Loss: {total_loss/len(train_loader):.3f} | Accuracy: {total_correct/len(train_loader):.3f}\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, writer=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, label in test_loader:\n",
    "            input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == label).sum().item()\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Loss/test\", total_loss/len(test_loader), epoch)\n",
    "        writer.add_scalar(\"Accuracy/test\", total_correct/len(test_loader), epoch)\n",
    "\n",
    "    print(f\"Test Epoch: {epoch} | Loss: {total_loss/len(test_loader):.3f} | Accuracy: {total_correct/len(test_loader):.3f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 | 11776 / 120000 [10 %] | Loss: 1.386 | Accuracy: 0.272\n",
      "Train Epoch: 0 | 23680 / 120000 [20 %] | Loss: 1.374 | Accuracy: 0.300\n",
      "Train Epoch: 0 | 35584 / 120000 [30 %] | Loss: 1.363 | Accuracy: 0.324\n",
      "Train Epoch: 0 | 47488 / 120000 [40 %] | Loss: 1.353 | Accuracy: 0.346\n",
      "Train Epoch: 0 | 59392 / 120000 [49 %] | Loss: 1.344 | Accuracy: 0.364\n",
      "Train Epoch: 0 | 71296 / 120000 [59 %] | Loss: 1.335 | Accuracy: 0.381\n",
      "Train Epoch: 0 | 83200 / 120000 [69 %] | Loss: 1.327 | Accuracy: 0.396\n",
      "Train Epoch: 0 | 95104 / 120000 [79 %] | Loss: 1.319 | Accuracy: 0.409\n",
      "Train Epoch: 0 | 107008 / 120000 [89 %] | Loss: 1.311 | Accuracy: 0.422\n",
      "Train Epoch: 0 | 118912 / 120000 [99 %] | Loss: 1.304 | Accuracy: 0.434\n",
      "Epoch: 0 | Loss: 1.303 | Accuracy: 55.650\n",
      "Test Epoch: 0 | Loss: 1.209 | Accuracy: 79.217\n",
      "Model saved at e:\\VSCODE\\Python\\DL-Hw\\BERT_medium_0.pth\n",
      "Train Epoch: 1 | 11776 / 120000 [10 %] | Loss: 1.225 | Accuracy: 0.554\n",
      "Train Epoch: 1 | 23680 / 120000 [20 %] | Loss: 1.218 | Accuracy: 0.563\n",
      "Train Epoch: 1 | 35584 / 120000 [30 %] | Loss: 1.212 | Accuracy: 0.569\n",
      "Train Epoch: 1 | 47488 / 120000 [40 %] | Loss: 1.206 | Accuracy: 0.575\n",
      "Train Epoch: 1 | 59392 / 120000 [49 %] | Loss: 1.201 | Accuracy: 0.579\n",
      "Train Epoch: 1 | 71296 / 120000 [59 %] | Loss: 1.195 | Accuracy: 0.584\n",
      "Train Epoch: 1 | 83200 / 120000 [69 %] | Loss: 1.189 | Accuracy: 0.587\n",
      "Train Epoch: 1 | 95104 / 120000 [79 %] | Loss: 1.184 | Accuracy: 0.592\n",
      "Train Epoch: 1 | 107008 / 120000 [89 %] | Loss: 1.179 | Accuracy: 0.597\n",
      "Train Epoch: 1 | 118912 / 120000 [99 %] | Loss: 1.173 | Accuracy: 0.602\n",
      "Epoch: 1 | Loss: 1.173 | Accuracy: 77.059\n",
      "Test Epoch: 1 | Loss: 1.093 | Accuracy: 90.533\n",
      "Train Epoch: 2 | 11776 / 120000 [10 %] | Loss: 1.115 | Accuracy: 0.648\n",
      "Train Epoch: 2 | 23680 / 120000 [20 %] | Loss: 1.109 | Accuracy: 0.653\n",
      "Train Epoch: 2 | 35584 / 120000 [30 %] | Loss: 1.106 | Accuracy: 0.653\n",
      "Train Epoch: 2 | 47488 / 120000 [40 %] | Loss: 1.101 | Accuracy: 0.658\n",
      "Train Epoch: 2 | 59392 / 120000 [49 %] | Loss: 1.097 | Accuracy: 0.660\n",
      "Train Epoch: 2 | 71296 / 120000 [59 %] | Loss: 1.092 | Accuracy: 0.661\n",
      "Train Epoch: 2 | 83200 / 120000 [69 %] | Loss: 1.087 | Accuracy: 0.664\n",
      "Train Epoch: 2 | 95104 / 120000 [79 %] | Loss: 1.083 | Accuracy: 0.665\n",
      "Train Epoch: 2 | 107008 / 120000 [89 %] | Loss: 1.079 | Accuracy: 0.667\n",
      "Train Epoch: 2 | 118912 / 120000 [99 %] | Loss: 1.075 | Accuracy: 0.669\n",
      "Epoch: 2 | Loss: 1.074 | Accuracy: 85.603\n",
      "Test Epoch: 2 | Loss: 1.000 | Accuracy: 93.633\n",
      "Train Epoch: 3 | 11776 / 120000 [10 %] | Loss: 1.021 | Accuracy: 0.692\n",
      "Train Epoch: 3 | 23680 / 120000 [20 %] | Loss: 1.020 | Accuracy: 0.691\n",
      "Train Epoch: 3 | 35584 / 120000 [30 %] | Loss: 1.019 | Accuracy: 0.690\n",
      "Train Epoch: 3 | 47488 / 120000 [40 %] | Loss: 1.017 | Accuracy: 0.690\n",
      "Train Epoch: 3 | 59392 / 120000 [49 %] | Loss: 1.013 | Accuracy: 0.692\n",
      "Train Epoch: 3 | 71296 / 120000 [59 %] | Loss: 1.009 | Accuracy: 0.693\n",
      "Train Epoch: 3 | 83200 / 120000 [69 %] | Loss: 1.007 | Accuracy: 0.694\n",
      "Train Epoch: 3 | 95104 / 120000 [79 %] | Loss: 1.003 | Accuracy: 0.695\n",
      "Train Epoch: 3 | 107008 / 120000 [89 %] | Loss: 1.000 | Accuracy: 0.696\n",
      "Train Epoch: 3 | 118912 / 120000 [99 %] | Loss: 0.997 | Accuracy: 0.697\n",
      "Epoch: 3 | Loss: 0.996 | Accuracy: 89.249\n",
      "Test Epoch: 3 | Loss: 0.926 | Accuracy: 96.167\n",
      "Train Epoch: 4 | 11776 / 120000 [10 %] | Loss: 0.964 | Accuracy: 0.706\n",
      "Train Epoch: 4 | 23680 / 120000 [20 %] | Loss: 0.957 | Accuracy: 0.711\n",
      "Train Epoch: 4 | 35584 / 120000 [30 %] | Loss: 0.953 | Accuracy: 0.712\n",
      "Train Epoch: 4 | 47488 / 120000 [40 %] | Loss: 0.951 | Accuracy: 0.712\n",
      "Train Epoch: 4 | 59392 / 120000 [49 %] | Loss: 0.948 | Accuracy: 0.712\n",
      "Train Epoch: 4 | 71296 / 120000 [59 %] | Loss: 0.946 | Accuracy: 0.713\n",
      "Train Epoch: 4 | 83200 / 120000 [69 %] | Loss: 0.943 | Accuracy: 0.713\n",
      "Train Epoch: 4 | 95104 / 120000 [79 %] | Loss: 0.941 | Accuracy: 0.714\n",
      "Train Epoch: 4 | 107008 / 120000 [89 %] | Loss: 0.937 | Accuracy: 0.714\n",
      "Train Epoch: 4 | 118912 / 120000 [99 %] | Loss: 0.935 | Accuracy: 0.714\n",
      "Epoch: 4 | Loss: 0.935 | Accuracy: 91.412\n",
      "Test Epoch: 4 | Loss: 0.865 | Accuracy: 97.833\n",
      "Train Epoch: 5 | 11776 / 120000 [10 %] | Loss: 0.904 | Accuracy: 0.722\n",
      "Train Epoch: 5 | 23680 / 120000 [20 %] | Loss: 0.903 | Accuracy: 0.725\n",
      "Train Epoch: 5 | 35584 / 120000 [30 %] | Loss: 0.902 | Accuracy: 0.724\n",
      "Train Epoch: 5 | 47488 / 120000 [40 %] | Loss: 0.899 | Accuracy: 0.723\n",
      "Train Epoch: 5 | 59392 / 120000 [49 %] | Loss: 0.897 | Accuracy: 0.724\n",
      "Train Epoch: 5 | 71296 / 120000 [59 %] | Loss: 0.895 | Accuracy: 0.723\n",
      "Train Epoch: 5 | 83200 / 120000 [69 %] | Loss: 0.892 | Accuracy: 0.724\n",
      "Train Epoch: 5 | 95104 / 120000 [79 %] | Loss: 0.890 | Accuracy: 0.725\n",
      "Train Epoch: 5 | 107008 / 120000 [89 %] | Loss: 0.888 | Accuracy: 0.725\n",
      "Train Epoch: 5 | 118912 / 120000 [99 %] | Loss: 0.886 | Accuracy: 0.726\n",
      "Epoch: 5 | Loss: 0.886 | Accuracy: 92.839\n",
      "Test Epoch: 5 | Loss: 0.817 | Accuracy: 98.333\n",
      "Model saved at e:\\VSCODE\\Python\\DL-Hw\\BERT_medium_5.pth\n",
      "Train Epoch: 6 | 11776 / 120000 [10 %] | Loss: 0.863 | Accuracy: 0.729\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\VSCODE\\Python\\DL-Hw\\BERT.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m latest_model_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     latest_model_epoch \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train(model, DEVICE, train_dataloader, optimizer, epoch, writer\u001b[39m=\u001b[39;49mwriter)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m test(model, DEVICE, test_dataloader, epoch, writer\u001b[39m=\u001b[39mwriter)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32me:\\VSCODE\\Python\\DL-Hw\\BERT.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m total_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (output\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m label)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/VSCODE/Python/DL-Hw/BERT.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m (batch_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(train_loader)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mprint_every) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"runs/BERT/medium/\")\n",
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    if latest_model_epoch:\n",
    "        epoch += latest_model_epoch + 1\n",
    "        latest_model_epoch = None\n",
    "\n",
    "    train(model, DEVICE, train_dataloader, optimizer, criterion, epoch, writer=writer)\n",
    "    test(model, DEVICE, test_dataloader, criterion, epoch, writer=writer)\n",
    "    if epoch % 2 == 0:\n",
    "        save_model(model, os.path.join(os.getcwd(), \"BERT_medium_\" + str(epoch) + \".pth\"))\n",
    "writer.close()\n",
    "\n",
    "save_model(model, os.path.join(os.getcwd(), \"BERT_medium_\" + str(epoch) + \".pth\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
